{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load input (features) and output (targets)\n",
        "data = np.load(\"X.npy\")  # Shape: (samples, time_steps, features)\n",
        "target = np.load(\"ynpy.npy\")  # Shape: (samples, output_dim)\n",
        "\n",
        "print(f\"Data shape: {data.shape}\")  # Should be (samples, 5, 3)\n",
        "print(f\"Target shape: {target.shape}\")  # Should be (samples, 3)"
      ],
      "metadata": {
        "id": "wQnjQRuBdGlH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n"
      ],
      "metadata": {
        "id": "byChLNRSYkdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "def create_time_windows(data, target, input_window=5):\n",
        "    \"\"\"\n",
        "    Convert long time-series data into overlapping sequences for LSTM training.\n",
        "\n",
        "    Args:\n",
        "    - data (np.array): Input data of shape (runs, timesteps, features)\n",
        "    - target (np.array): Target data of shape (runs, timesteps, features)\n",
        "    - input_window (int): Number of time steps per sequence\n",
        "\n",
        "    Returns:\n",
        "    - X: Input sequences of shape (samples, input_window, features)\n",
        "    - y: Corresponding target values of shape (samples, output_dim)\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "\n",
        "    num_runs, timesteps, num_features = data.shape\n",
        "\n",
        "    for run in range(num_runs):\n",
        "        for i in range(timesteps - input_window):\n",
        "            # Extract input time window\n",
        "            X.append(data[run, i:i+input_window, :])\n",
        "            # Extract target (next time step after window)\n",
        "            y.append(target[run, i+input_window, :])\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Convert data into time-windowed sequences\n",
        "X, y = create_time_windows(data, target, input_window=5)\n",
        "\n",
        "print(f\"Processed Data Shape: {X.shape}\")  # (samples, 5, 3)\n",
        "print(f\"Processed Labels Shape: {y.shape}\")  # (samples, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWgW5FT8ZVCu",
        "outputId": "7162f84a-e344-4458-f637-79fc52c4c720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Data Shape: (391250, 5, 3)\n",
            "Processed Labels Shape: (391250, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Initialize scalers\n",
        "scaler = MinMaxScaler()  # Use MinMaxScaler for GPS coordinates\n",
        "imu_scaler = StandardScaler()  # Use StandardScaler for IMU data if necessary\n",
        "\n",
        "# Flatten the data to fit the scaler\n",
        "X_flattened = X.reshape(-1, X.shape[-1])\n",
        "y_flattened = y.reshape(-1, y.shape[-1])\n",
        "\n",
        "# Apply normalization\n",
        "X_scaled = scaler.fit_transform(X_flattened)\n",
        "y_scaled = scaler.fit_transform(y_flattened)\n",
        "\n",
        "# Reshape back to original dimensions\n",
        "X_scaled = X_scaled.reshape(X.shape)\n",
        "y_scaled = y_scaled.reshape(y.shape)"
      ],
      "metadata": {
        "id": "tRE_usHMh-CQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data (80% Train, 20% Test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "# Print shapes\n",
        "print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}, y_test shape: {y_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FjoFWOOd_6E",
        "outputId": "708a74ba-a6e5-4814-b770-481baca72a51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (313000, 5, 3), X_test shape: (78250, 5, 3)\n",
            "y_train shape: (313000, 3), y_test shape: (78250, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Dot, Activation, Concatenate, Flatten\n",
        "\n",
        "# Define input shapes\n",
        "time_steps = X.shape[1]  # Sequence length (past 5 seconds of data)\n",
        "num_features = X.shape[2]  # Number of input features per time step (IMU + GPS)\n",
        "output_dim = y.shape[1]\n",
        "# Define inputs\n",
        "inputs = Input(shape=(time_steps, num_features), name=\"Input_Features\")\n",
        "\n",
        "# Encoder (LSTM)\n",
        "encoder_lstm = LSTM(64, return_sequences=True, return_state=True, name=\"Encoder_LSTM\")\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(inputs)\n",
        "\n",
        "# Attention Mechanism (Local Attention)\n",
        "# Calculate attention scores (alignment scores)\n",
        "attention_scores = Dense(1, activation='tanh', name=\"Attention_Scores\")(encoder_outputs)\n",
        "\n",
        "#Normalize scores to obtain attention weights\n",
        "attention_weights = Activation('softmax', name=\"Attention_Weights\")(attention_scores)\n",
        "\n",
        "#Compute the context vector as the weighted sum of encoder outputs\n",
        "context_vector = Dot(axes=1, name=\"Context_Vector\")([attention_weights, encoder_outputs])\n",
        "\n",
        "# Decoder (Fully Connected Layer)\n",
        "decoder_dense = Dense(64, activation='relu', name=\"Decoder_Dense\")(context_vector)\n",
        "\n",
        "# Output Layer (Latitude, Longitude, Altitude)\n",
        "output_layer = Dense(3, activation='linear', name=\"Trajectory_Output\")(decoder_dense)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=output_layer, name=\"Trajectory_Prediction_Model\")\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "Mdc6JhjTeI2q",
        "outputId": "99fbc3f6-8c58-4110-bc79-c3ba2088c886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"Trajectory_Prediction_Model\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Trajectory_Prediction_Model\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ Input_Features            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Encoder_LSTM (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, │         \u001b[38;5;34m17,408\u001b[0m │ Input_Features[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│                           │ \u001b[38;5;34m64\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)]       │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Attention_Scores (\u001b[38;5;33mDense\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m1\u001b[0m)           │             \u001b[38;5;34m65\u001b[0m │ Encoder_LSTM[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Attention_Weights         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m1\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ Attention_Scores[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Context_Vector (\u001b[38;5;33mDot\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │              \u001b[38;5;34m0\u001b[0m │ Attention_Weights[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                           │                        │                │ Encoder_LSTM[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Decoder_Dense (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m4,160\u001b[0m │ Context_Vector[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Trajectory_Output (\u001b[38;5;33mDense\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │            \u001b[38;5;34m195\u001b[0m │ Decoder_Dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ Input_Features            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Encoder_LSTM (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, │         <span style=\"color: #00af00; text-decoration-color: #00af00\">17,408</span> │ Input_Features[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)]       │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Attention_Scores (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ Encoder_LSTM[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Attention_Weights         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ Attention_Scores[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Context_Vector (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dot</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ Attention_Weights[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                           │                        │                │ Encoder_LSTM[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Decoder_Dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │ Context_Vector[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Trajectory_Output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │            <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │ Decoder_Dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m21,828\u001b[0m (85.27 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,828</span> (85.27 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m21,828\u001b[0m (85.27 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,828</span> (85.27 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Early stopping to monitor validation loss and stop if no improvement\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=150,  # Maximum epochs\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,  # 20% of training data for validation\n",
        "    callbacks=[early_stopping]  # Use early stopping\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "model.save('trajectory_prediction_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnq_eSeLZXoO",
        "outputId": "b8519e76-4435-48e7-972f-38c6d868b881"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 5, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 0.0943 - mae: 0.2505 - val_loss: 0.0874 - val_mae: 0.2443\n",
            "Epoch 2/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 6ms/step - loss: 0.0877 - mae: 0.2439 - val_loss: 0.0875 - val_mae: 0.2408\n",
            "Epoch 3/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6ms/step - loss: 0.0875 - mae: 0.2433 - val_loss: 0.0874 - val_mae: 0.2414\n",
            "Epoch 4/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6ms/step - loss: 0.0876 - mae: 0.2439 - val_loss: 0.0874 - val_mae: 0.2450\n",
            "Epoch 5/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6ms/step - loss: 0.0877 - mae: 0.2440 - val_loss: 0.0876 - val_mae: 0.2453\n",
            "Epoch 6/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - loss: 0.0875 - mae: 0.2439 - val_loss: 0.0874 - val_mae: 0.2431\n",
            "Epoch 7/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6ms/step - loss: 0.0876 - mae: 0.2439 - val_loss: 0.0874 - val_mae: 0.2459\n",
            "Epoch 8/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6ms/step - loss: 0.0876 - mae: 0.2437 - val_loss: 0.0874 - val_mae: 0.2433\n",
            "Epoch 9/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 6ms/step - loss: 0.0876 - mae: 0.2440 - val_loss: 0.0874 - val_mae: 0.2442\n",
            "Epoch 10/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 6ms/step - loss: 0.0875 - mae: 0.2436 - val_loss: 0.0874 - val_mae: 0.2424\n",
            "Epoch 11/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - loss: 0.0876 - mae: 0.2440 - val_loss: 0.0873 - val_mae: 0.2439\n",
            "Epoch 12/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 6ms/step - loss: 0.0876 - mae: 0.2439 - val_loss: 0.0874 - val_mae: 0.2433\n",
            "Epoch 13/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 6ms/step - loss: 0.0873 - mae: 0.2434 - val_loss: 0.0874 - val_mae: 0.2439\n",
            "Epoch 14/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - loss: 0.0876 - mae: 0.2438 - val_loss: 0.0874 - val_mae: 0.2422\n",
            "Epoch 15/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - loss: 0.0876 - mae: 0.2439 - val_loss: 0.0874 - val_mae: 0.2453\n",
            "Epoch 16/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6ms/step - loss: 0.0876 - mae: 0.2440 - val_loss: 0.0874 - val_mae: 0.2452\n",
            "Epoch 17/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6ms/step - loss: 0.0874 - mae: 0.2435 - val_loss: 0.0874 - val_mae: 0.2422\n",
            "Epoch 18/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6ms/step - loss: 0.0876 - mae: 0.2438 - val_loss: 0.0874 - val_mae: 0.2445\n",
            "Epoch 19/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6ms/step - loss: 0.0877 - mae: 0.2440 - val_loss: 0.0874 - val_mae: 0.2429\n",
            "Epoch 20/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6ms/step - loss: 0.0876 - mae: 0.2440 - val_loss: 0.0873 - val_mae: 0.2433\n",
            "Epoch 21/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6ms/step - loss: 0.0876 - mae: 0.2440 - val_loss: 0.0873 - val_mae: 0.2431\n",
            "Epoch 22/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 7ms/step - loss: 0.0876 - mae: 0.2439 - val_loss: 0.0874 - val_mae: 0.2442\n",
            "Epoch 23/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 6ms/step - loss: 0.0877 - mae: 0.2441 - val_loss: 0.0874 - val_mae: 0.2437\n",
            "Epoch 24/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6ms/step - loss: 0.0875 - mae: 0.2437 - val_loss: 0.0873 - val_mae: 0.2443\n",
            "Epoch 25/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6ms/step - loss: 0.0874 - mae: 0.2436 - val_loss: 0.0874 - val_mae: 0.2431\n",
            "Epoch 26/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6ms/step - loss: 0.0875 - mae: 0.2439 - val_loss: 0.0874 - val_mae: 0.2440\n",
            "Epoch 27/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6ms/step - loss: 0.0876 - mae: 0.2440 - val_loss: 0.0873 - val_mae: 0.2438\n",
            "Epoch 28/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6ms/step - loss: 0.0874 - mae: 0.2435 - val_loss: 0.0874 - val_mae: 0.2430\n",
            "Epoch 29/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6ms/step - loss: 0.0875 - mae: 0.2437 - val_loss: 0.0874 - val_mae: 0.2432\n",
            "Epoch 30/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 6ms/step - loss: 0.0876 - mae: 0.2437 - val_loss: 0.0873 - val_mae: 0.2425\n",
            "Epoch 31/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - loss: 0.0874 - mae: 0.2437 - val_loss: 0.0873 - val_mae: 0.2434\n",
            "Epoch 32/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6ms/step - loss: 0.0876 - mae: 0.2439 - val_loss: 0.0874 - val_mae: 0.2420\n",
            "Epoch 33/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6ms/step - loss: 0.0877 - mae: 0.2442 - val_loss: 0.0873 - val_mae: 0.2430\n",
            "Epoch 34/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6ms/step - loss: 0.0874 - mae: 0.2435 - val_loss: 0.0874 - val_mae: 0.2424\n",
            "Epoch 35/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 7ms/step - loss: 0.0874 - mae: 0.2435 - val_loss: 0.0874 - val_mae: 0.2441\n",
            "Epoch 36/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 6ms/step - loss: 0.0875 - mae: 0.2437 - val_loss: 0.0873 - val_mae: 0.2439\n",
            "Epoch 37/150\n",
            "\u001b[1m3913/3913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - loss: 0.0875 - mae: 0.2439 - val_loss: 0.0874 - val_mae: 0.2447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_mae = model.evaluate(X_test, y_test, verbose=1)\n",
        "\n",
        "print(f\"Test Loss (MSE): {test_loss}\")\n",
        "print(f\"Test Mean Absolute Error (MAE): {test_mae}\")"
      ],
      "metadata": {
        "id": "4WLZt4kMbidf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Predict the test set\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = np.squeeze(y_pred)\n",
        "# Compare predictions with ground truth\n",
        "print(\"Predicted values:\", y_pred[:5])  # First 5 predictions\n",
        "print(\"Actual values:\", y_test[:5])    # First 5 ground truth values\n",
        "\n",
        "# Plotting predictions vs. actual values\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(y_test[:, 0], label=\"Actual Latitude\", color='blue')  # Latitude\n",
        "plt.plot(y_pred[:, 0], label=\"Predicted Latitude\", color='cyan', linestyle='dashed')\n",
        "plt.title(\"Latitude: Actual vs Predicted\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Similarly for Longitude and Altitude\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(y_test[:, 1], label=\"Actual Longitude\", color='green')  # Longitude\n",
        "plt.plot(y_pred[:, 1], label=\"Predicted Longitude\", color='lime', linestyle='dashed')\n",
        "plt.title(\"Longitude: Actual vs Predicted\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(y_test[:, 2], label=\"Actual Altitude\", color='red')  # Altitude\n",
        "plt.plot(y_pred[:, 2], label=\"Predicted Altitude\", color='orange', linestyle='dashed')\n",
        "plt.title(\"Altitude: Actual vs Predicted\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Pyc2WVHAcgfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Compute MAE, RMSE, and R^2 score for Latitude, Longitude, and Altitude\n",
        "mae = mean_absolute_error(y_test, y_pred, multioutput='raw_values')\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred, multioutput='raw_values'))\n",
        "r2 = r2_score(y_test, y_pred, multioutput='raw_values')\n",
        "\n",
        "# Print results\n",
        "print(f\"Mean Absolute Error (MAE): Latitude={mae[0]:.6f}, Longitude={mae[1]:.6f}, Altitude={mae[2]:.6f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): Latitude={rmse[0]:.6f}, Longitude={rmse[1]:.6f}, Altitude={rmse[2]:.6f}\")\n",
        "print(f\"R² Score: Latitude={r2[0]:.6f}, Longitude={r2[1]:.6f}, Altitude={r2[2]:.6f}\")"
      ],
      "metadata": {
        "id": "4WUyuPB3dAXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cS6R_P8gZQCx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}